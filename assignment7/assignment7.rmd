---
title: "BSDA: Assignment 7"
author: "Anonymous student"
output: 
  pdf_document: 
    toc: yes
    toc_depth: 1
header-includes:
- \usepackage{float}
- \usepackage{graphicx}
- \usepackage{amsmath}
- \usepackage{relsize}
- \usepackage{cancel}
- \usepackage{booktabs}
- \DeclareMathOperator{\E}{E}
- \DeclareMathOperator{\Var}{Var}
- \DeclareMathOperator{\var}{var}
- \DeclareMathOperator{\Sd}{Sd}
- \DeclareMathOperator{\sd}{sd}
- \DeclareMathOperator{\Bin}{Bin}
- \DeclareMathOperator{\Beta}{Beta}
- \DeclareMathOperator{\Poisson}{Poisson}
- \DeclareMathOperator{\betacdf}{betacdf}
- \DeclareMathOperator{\Invchi2}{Inv-\chi^2}
- \DeclareMathOperator{\logit}{logit}
- \DeclareMathOperator{\N}{N}
- \DeclareMathOperator{\U}{U}
- \DeclareMathOperator{\tr}{tr}
- \DeclareMathOperator{\trace}{trace}
editor_options: 
  markdown: 
    wrap: 72
---

\newcommand{\vc}[1] { \mathbf{#1} }
\newcommand{\vs}[1] { \boldsymbol{#1} }

# General Information to include

-   **Time used for reading and self-study exercises**: \~ 8 hours.
-   **Time used for the assignment**: \~12 hours
-   **Good with assignment**: It was good to see how hierarchical models are implemented in Stan
-   **Things to improve in the assignment**: It was a really difficult and demanding assignment, I felt like only with what we saw in the lecture wasn't enough to solve the whole assignment.

```{r setup, include=FALSE}
# This chunk sets echo = TRUE as default, that is, print all code.
# knitr::opts_chunk$set can be used to set other notebook generation options, too.
# include=FALSE inside curly brackets makes this block not be included in the pdf.
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Linear model: drowning data with Stan

The provided data \texttt{drowning} in the \texttt{bsda} package contains the number of people who died from drowning each year in Finland 1980--2019.
%
A statistician is going to fit a linear model with Gaussian residual model to these data using time as the predictor and number of drownings as the target variable (see the related linear model example for the Kilpisj√§rvi-temperature data in the example Stan codes).
She has two objective questions:

\begin{itemize}
    \item [i)] What is the trend of the number of people drowning per year? (We would plot the histogram of the slope of the linear model.)
    \item [ii)] What is the prediction for the year 2020? (We would plot the histogram of the posterior predictive distribution for the number of people drowning at $\tilde x=2020$.)
\end{itemize}

To access the data, use:

```{r, warning=FALSE, message=FALSE}
library(bsda)
library(tigerstats)
library(rjson)
library(rstan)
library(lubridate)
library(ggplot2)
library(tidyverse)
library(knitr)
library(kableExtra)
library(posterior)
library(bayesplot)
library(bayestestR)
data("drowning")
drowning<- drowning
```

Corresponding Stan code is provided in Listing 1. However, it is not entirely correct for the problem. First, there are \emph{three mistakes}. Second, there are no priors defined for the parameters. In Stan, this corresponds to using uniform priors. 

Your tasks are the following:


## a) Find the three mistakes in the code and fix them. Report the original mistakes and your fixes clearly in your report. Include the \emph{full} corrected Stan code in your report.

I am reporting all the modifications to the Stan Code using code chunks in this report. I also created a Stan file code \texttt{assignment7\_1.stan} with all these modifications which I use in the \texttt{stan()} function.

This is the corrected Stan code:

```{js}
data {
    int<lower=0> N;  // number of data points
    vector[N] x;     // observation year
    vector[N] y;     // observation number of drowned
    real xpred;      // prediction year
}
parameters {
    real alpha;
    real beta;
    real<lower=0> sigma; // fixed upper to lower, sigma can only take values>0 
}
transformed parameters {
    vector[N] mu = alpha + beta*x;
}
model {
    y ~ normal(mu, sigma); // fixed by adding ;
}
generated quantities {
    real ypred = normal_rng(alpha + beta*xpred, sigma); //fixed using xpred instead of x
}
```

## b) Determine a suitable weakly-informative prior $\operatorname{normal}(0, \sigma_\beta)$ for the slope \texttt{beta}. It is very unlikely that the mean number of drownings changes more than 50 \% in one year. The approximate historical mean yearly number of drownings is 138. Hence, set $\sigma_\beta$ so that the following holds for the prior probability for \texttt{beta}: $\operatorname{Pr}(-69 < \text{\texttt{beta}} < 69) = 0.99$. Determine suitable value for $\sigma_\beta$ and report the approximate numerical value for it.

Given that we are using a weakly-informative prior for the slope $\beta \sim N(0,\sigma_\beta)$ and that we know that the historical mean yearly drownings is 138 and it doesn't varies more than 50% in a year. We can set the following condition for sigma $\operatorname{Pr}(-69 < \text{\texttt{beta}} < 69) = 0.99$. Then we can normalize

$$
\begin{aligned}
\left( \frac{-69-\mu_\beta}{\sigma_\beta} < \frac{\beta - \mu_\beta}{\sigma_\beta} <  \frac{69-\mu_\beta}{\sigma_\beta}\right) &= 0.99 \\
\left( \frac{-69-0}{\sigma_\beta} < Z <  \frac{69-0}{\sigma_\beta}\right) &= 0.99
\end{aligned}
$$
Because of symmetry in the normal distribution we have:

```{r, fig.height= 4}
z = round(qnorm(0.005,mean=0,sd=1), 3)

pnormGC(c(z,-z),region="between",mean=0,
        sd=1,graph=TRUE)
```

Then solving for $\sigma_\beta$ we get:

$$
\begin{aligned}
\frac{-69}{\sigma_\beta} &= `r z`  \Rightarrow \sigma_\beta = `r round(-69/qnorm(0.005,mean=0,sd=1),3)` 
\end{aligned}
$$

Therefore, a suitable value for $\sigma_\beta$ could be $\sigma_\beta = `r round(-69/qnorm(0.005,mean=0,sd=1))`$.

##  Using the obtained $\sigma_\beta$, add the desired prior in the Stan code. In the report, in a separate section, indicate clearly how you carried out your prior implementation, e.g.\ ``Added line \dots in block \dots''

I added the following line in the model section and marked it with ***:

```{js}
beta ~ normal(0, 27);
```

```{js}
data {
    int<lower=0> N;  // number of data points
    vector[N] x;     // observation year
    vector[N] y;     // observation number of drowned
    real xpred;      // prediction year
}
parameters {
    real alpha;
    real beta;
    real<lower=0> sigma; // fixed upper to lower, sigma can only take values>0 
}
transformed parameters {
    vector[N] mu = alpha + beta*x;
}
model {
    y ~ normal(mu, sigma); // fixed by adding ;
    beta ~ normal(0, 27); // *** added the beta prior here 
}
generated quantities {
    real ypred = normal_rng(alpha + beta*xpred, sigma); //fixed using xpred instead of x
}
```


## d) In a similar way, add a weakly informative prior for the intercept \texttt{alpha} and explain how you chose the prior.

I used the data mean and sd to choose a weakly informative prior for the intercept. 

```{r}
summary(drowning$drownings)
sd(drowning$drownings)
```

By looking at these statistics we can choose a prior for the intercept centered about the data mean number of drownings and have most of its mass between `r min(drowning$drownings)` and  `r max(drowning$drownings)`. Therefore we can choose a weakly informative prior like $\alpha \sim N(`r round(mean(drowning$drownings))`, 55)$ 

I added the following line in the model section and marked it with ***:

```{js}
alpha ~ normal(134, 35);
```


```{js}
data {
    int<lower=0> N;  // number of data points
    vector[N] x;     // observation year
    vector[N] y;     // observation number of drowned
    real xpred;      // prediction year
}
parameters {
    real alpha;
    real beta;
    real<lower=0> sigma; // fixed upper to lower, sigma can only take values>0 
}
transformed parameters {
    vector[N] mu = alpha + beta*x;
}
model {
    y ~ normal(mu, sigma); // fixed by adding ;
    beta ~ normal(0, 27); // added the beta prior here 
    alpha ~ normal(134, 35); // *** added the alpha prior here
}
generated quantities {
    real ypred = normal_rng(alpha + beta*xpred, sigma); //fixed using xpred instead of x
}
```

Now we can run our model

```{r, message=FALSE, warning=FALSE}
options(mc.cores = parallel::detectCores())

fit1 <- stan(file = 'assignment7_1.stan', 
             data = list(N = nrow(drowning),
                         x = drowning$year - mean(drowning$year),
                         y = drowning$drownings,
                         xpred = 2020 - mean(drowning$year)
                         ), 
             seed = 123
             )
```

```{r, warning=FALSE, message=FALSE, fig.height= 4}
df_posterior <- bayestestR::describe_posterior(fit1, ci = 0.95, test = c()) %>%
  as_tibble() %>%
  filter(str_detect(Parameter, "mu")) %>%
  select(Parameter, Median, CI_low, CI_high) 
df_posterior <- cbind(df_posterior, drowning)

summary_fit1 <- summary(fit1)
alpha_est <- summary_fit1$summary["alpha", "mean"]
beta_est <- summary_fit1$summary["beta", "mean"]

ggplot(df_posterior, aes(x = year, y = drownings)) +
  geom_point() + geom_line(aes(x = year, 
                               y = alpha_est + beta_est * (year-mean(year))), 
                           color = "blue") +
  geom_smooth(aes(y = CI_low),
              method = "loess",formula = "y ~ x", se = FALSE,
              color = 'blue', linetype = 2) +
  geom_smooth(aes(y = CI_high),
              method = "loess",formula = "y ~ x", se = FALSE,
              color = 'blue', linetype = 2) +
  labs(x = "Year", y = "Drownings", title = "Linear Model Fit with 95% CI") +
  theme_classic()

stan_hist(fit1, pars = c('beta', 'ypred'), nrow = 2)
```

# 2. Hierarchical model: factory data with Stan

The \texttt{factory} data in the \texttt{bsda} package contains quality control measurements from 6 machines in a factory. In the data file, each column contains the measurements for a single machine. Quality control measurements are expensive and time-consuming, so only 5 measurements were done for each machine. In addition to the existing machines, we are interested in the quality of another machine (the seventh machine). To read in the data, just use:

```{r}
data("factory")
```

For this problem, you'll use the following Gaussian models:
\begin{itemize}
\item a separate model, in which each machine has its own model/parameters
\item a pooled model, in which all measurements are combined and there is no distinction between machines
\item a hierarchical model, which has a hierarchical structure as described in BDA3 Section 11.6.
\end{itemize}

As in the model described in the book, use the same measurement standard deviation $\sigma$ for all the groups in the hierarchical model. In the separate model, however, use separate measurement standard deviation $\sigma_j$ for each group $j$. You should use weakly informative priors for all your models.

## a) Describe the model with mathematical notation (as is done for the separate model above). Also describe in words the difference between the three models.

Separate model: this model considers each of the machines separately, building a separate model for each of them. In this case, each machine has its own $\mu_j$ and $\sigma_j$ parameters.

$$
\begin{aligned}
    y_{ij} &\sim N(\mu_j,\sigma_j)\\
    \mu_{j} &\sim N(95,20)\\
    \sigma_{j} &\sim \Invchi2(10)
\end{aligned}
$$

Pooled model: in this model all the data from different machines are combined into a single model. This means that we treat all observations as if they came from a single group. In this case, there is a single $\mu$ and $\sigma$ for all of the data.

$$
\begin{aligned}
    y_{i} &\sim N(\mu,\sigma)\\
    \mu_{} &\sim N(95,20)\\
    \sigma_{} &\sim \Invchi2(10)
\end{aligned}
$$

Hierarchical model: this model allows for differences between groups but also assumes that there is some underlying structure that connects them, with shared parameters at the group level. In this case, each machine has its own $\mu_j$ and $\sigma_j$ parameters, but all of them are drawn from a common distribution, therefore they all have common hyperparameters $\mu$ and $\tau$.

Using [Stan Prior Choice Recommendations](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations#prior-for-scale-parameters-in-hierarchical-models)
$$
\begin{aligned}
    y_{ij} &\sim N(\mu_{j},\sigma_j)\\
    \mu_{j} &\sim N(\mu,\tau)\\
    \mu &\sim N(95,20)\\
    \tau &\sim Half-t(4,0,20)\\
    \sigma_{j} &\sim \Invchi2(10)
\end{aligned}
$$

## b) Implement the model in Stan and include the code in the report. Use weakly informative priors for all your models.

Separate model:

Here is the Stan code from \texttt{assignment7\_separate.stan}

```{js}
data {
  int<lower=0> N;
  int<lower=0> J;
  vector[J] y[N];
}

parameters {
  vector[J] mu;
  vector<lower=0>[J] sigma;
}

model {
  // priors
  for (j in 1:J){
    mu[j] ~ normal(100, 10);
    sigma[j] ~ inv_chi_square(10);
  }

  // likelihood
  for (j in 1:J)
    y[,j] ~ normal(mu[j], sigma[j]);
}

generated quantities {
  real ypred;
  // Compute predictive distribution
  ypred6 = normal_rng(mu[6], sigma[6]);
}
```

Here is the R implementation

```{r}
options(mc.cores = parallel::detectCores())
sm_sep <- rstan::stan_model(file = "assignment7_separate.stan")

stan_data_separate <- list(
  y = factory,
  N = nrow(factory),
  J = ncol(factory)
)

model_separate <- rstan::sampling(sm_sep, data = stan_data_separate)

df_summary_sep <- summarise_draws(model_separate)

df_summary_sep %>%  kbl(booktabs = T, linesep = "") %>%
  kable_styling(latex_options = c("striped", "scale_down", "HOLD_position")) 
```

Pooled model:

Here is the Stan code from \texttt{assignment7\_pooled.stan}

```{js}
data {
  int<lower=0> N;
  vector[N] y;
}

parameters {
  real mu;
  real<lower=0> sigma;
}

model {
  // priors
  mu ~ normal(100, 10);
  sigma ~ inv_chi_square(10);

  // likelihood
  y ~ normal(mu, sigma);
}

generated quantities {
  real ypred;
  // Compute predictive distribution
  ypred = normal_rng(mu, sigma);
}
```

Here is the R implementation

```{r}
options(mc.cores = parallel::detectCores())
sm_pool <- rstan::stan_model(file = "assignment7_pooled.stan")

stan_data_pooled <- list(
  y = unname(unlist(factory)),
  N = nrow(factory) * ncol(factory)
)

model_pooled <- rstan::sampling(sm_pool, data = stan_data_pooled)

df_summary_pool <- summarise_draws(model_pooled) 

df_summary_pool %>%  kbl(booktabs = T, linesep = "") %>%
  kable_styling(latex_options = c("striped", "scale_down", "HOLD_position")) 

```

Hierarchical model:

Here is the Stan code from \texttt{assignment7\_h.stan}

```{js}
data {
  int<lower=0> N;
  int<lower=0> J;
  vector[J] y[N];
}

parameters {
  vector[J] mu;
  vector<lower=0>[J] sigma;
  real mu_p;
  real<lower=0> tau;
}

model {
  alpha ~ normal(95, 20);
  tau ~ student_t(4, 0, 20);
  // priors
  for (j in 1:J){
    mu[j] ~ normal(alpha, tau);
    sigma[j] ~ inv_chi_square(10);
  }

  // likelihood
  for (j in 1:J)
    y[,j] ~ normal(mu[j], sigma[j]);
}

generated quantities {
  real ypred1;
  real ypred2;
  real ypred3;
  real ypred4;
  real ypred5;
  real ypred6;
  real ypred7;
  real mu_pred7;
  real sigma_pred7;
  // Compute predictive distribution
  ypred1 = normal_rng(mu[1], sigma[1]);
  ypred2 = normal_rng(mu[2], sigma[2]);
  ypred3 = normal_rng(mu[3], sigma[3]);
  ypred4 = normal_rng(mu[4], sigma[4]);
  ypred5 = normal_rng(mu[5], sigma[5]);
  ypred6 = normal_rng(mu[6], sigma[6]);
  mu_pred7 = normal_rng(alpha, tau);
  sigma_pred7 = inv_chi_square_rng(10);
  ypred7 = normal_rng(mu_pred7, sigma_pred7);
}
```

Here is the R implementation

```{r}
options(mc.cores = parallel::detectCores())
sm_h <- rstan::stan_model(file = "assignment7_h.stan")

stan_data_h <- list(
  y = factory,
  N = nrow(factory),
  J = ncol(factory)
)

model_h <- rstan::sampling(sm_h, data = stan_data_h)

df_summary_h <- summarise_draws(model_h)

df_summary_h %>%  kbl(booktabs = T, linesep = "") %>%
  kable_styling(latex_options = c("striped", "scale_down", "HOLD_position")) 

```

## c) Using the model (with weakly informative priors) report, comment on and, if applicable, plot histograms for the following distributions:

### i) the posterior distribution of the mean of the quality measurements of the sixth machine.

In this case we can plot the histograms for all the posterior distributions of the mean of the quality measurements, note that in the case of the pooled model this histogram will always be the same no matter the machine, we are just plotting the pooled mean $\mu$, because we are assuming they all belong in the same single group. The histograms in both the hierarchical and separate model look really similar, but if we see our point estimates we can see that in the hierarchical model we get a little higher mean and a little lower sd of the mean of the quality measurements of the sixth machine.

```{r, warning=FALSE, message=FALSE, fig.height= 3}
stan_hist(model_separate, pars = c('mu[6]')) + ggtitle("Separate Model")
stan_hist(model_pooled, pars = c('mu')) + ggtitle("Pooled Model")
stan_hist(model_h, pars = c('mu[6]')) + ggtitle("Hierarchical Model")
```

### ii) the predictive distribution for another quality measurement of the sixth machine.

In this case we can also plot the histograms for all the predictive distributions for another quality measurement of the sixth machine, note that in the case of the pooled model this histogram is again always the same no matter the machine, we are just plotting the $\tilde y$. The histograms in both the hierarchical and separate model again look really similar, and our point estimates in both cases are also really close.

```{r, warning=FALSE, message=FALSE, fig.height= 3}
stan_hist(model_separate, pars = c('ypred6')) + ggtitle("Separate Model")
stan_hist(model_pooled, pars = c('ypred')) + ggtitle("Pooled Model")
stan_hist(model_h, pars = c('ypred6')) + ggtitle("Hierarchical Model")
```

### iii) the posterior distribution of the mean of the quality measurements of a seventh machine (not in the data).

Finally, In this case we cannot plot the histograms for all the posterior distribution of the mean of the quality measurements of a seventh machine, specifically we cannot do this for the separate model, since it considers each machine separately. On the contrary, in pooled models and hierarchical models we can get two types of predictive distributions: (i) A new observation in an existing group.(ii) A new observation in a new group. Note that in the case of the pooled model this histogram is the same as in question i),the pooled mean $\mu$.

```{r, warning=FALSE, message=FALSE, fig.height= 3}
stan_hist(model_pooled, pars = c('mu')) + ggtitle("Pooled Model")
stan_hist(model_h, pars = c('mu_pred7')) + ggtitle("Hierarchical Model")
```

## Report the posterior expectation for $\mu_1$ with a 90\% credible interval but using a $\operatorname{normal}(0,10)$ prior for the $\mu$ parameter(s) and a $\operatorname{Gamma}(1,1)$ prior for the $\sigma$ parameter(s). For the hierarchical model, use the $\operatorname{normal}(0,10)$ and $\operatorname{Gamma}(1,1)$ as hyper-priors.

The code to get these results can be consulted in the Appendix at the end. 


```{r, echo=FALSE}
options(mc.cores = parallel::detectCores())
sm_d_sep <- rstan::stan_model(file = "assignment7d_sep.stan")

stan_data_d_separate <- list(
  y = factory,
  N = nrow(factory),
  J = ncol(factory)
)

model_d_separate <- rstan::sampling(sm_d_sep, data = stan_data_d_separate)

m_s <- extract_variable_matrix(model_d_separate, variable = 'mu[1]')
q_s <- mcse_quantile(m_s, probs = c(0.10, 0.90))
              
df_summary_d_s <- summarise_draws(model_d_separate,mean,mcse_mean,
                                  ~quantile(.x, probs = c(0.1, 0.9))) %>% 
  filter(variable=='mu[1]') %>% rename(q10 = `10%`, q90 = `90%`)
df_summary_d_s$mcse_q10 <- q_s[1]
df_summary_d_s$mcse_q90 <- q_s[2]

df_summary_d_s %>%  kbl(booktabs = T, linesep = "", caption = "Separate Model") %>%
  kable_styling(latex_options = c("striped", "scale_down", "HOLD_position")) 
```

```{r, echo=FALSE}
options(mc.cores = parallel::detectCores())
sm_d_pool <- rstan::stan_model(file = "assignment7d_pool.stan")

stan_data_d_pooled <- list(
  y = unname(unlist(factory)),
  N = nrow(factory) * ncol(factory)
)

model_d_pooled <- rstan::sampling(sm_d_pool, data = stan_data_d_pooled)

m_p <- extract_variable_matrix(model_d_pooled, variable = 'mu')
q_p <- mcse_quantile(m_p, probs = c(0.10, 0.90))
              
df_summary_d_p <- summarise_draws(model_d_pooled, mean,mcse_mean,
                                  ~quantile(.x, probs = c(0.1, 0.9))) %>% 
  filter(variable=='mu') %>% rename(q10 = `10%`, q90 = `90%`)
df_summary_d_p$mcse_q10 <- q_p[1]
df_summary_d_p$mcse_q90 <- q_p[2]

df_summary_d_p %>%  kbl(booktabs = T, linesep = "", caption = "Pooled Model") %>%
  kable_styling(latex_options = c("striped", "scale_down", "HOLD_position")) 
```

```{r, echo=FALSE}
options(mc.cores = parallel::detectCores())
sm_d_h <- rstan::stan_model(file = "assignment7d_h.stan")

stan_data_d_h <- list(
  y = factory,
  N = nrow(factory),
  J = ncol(factory)
)

model_d_h <- rstan::sampling(sm_d_h, data = stan_data_d_h)

m_h <- extract_variable_matrix(model_d_h, variable = 'mu[1]')
q_h <- mcse_quantile(m_h, probs = c(0.10, 0.90))
              
df_summary_d_h <- summarise_draws(model_d_h,mean,mcse_mean,
                                  ~quantile(.x, probs = c(0.1, 0.9))) %>% 
  filter(variable=='mu[1]') %>% rename(q10 = `10%`, q90 = `90%`)
df_summary_d_h$mcse_q10 <- q_h[1]
df_summary_d_h$mcse_q90 <- q_h[2]

df_summary_d_h %>%  kbl(booktabs = T, linesep = "", caption = "Hierarchical Model") %>%
  kable_styling(latex_options = c("striped", "scale_down", "HOLD_position")) 
```

We can report the the posterior expectation for $\mu_1$ as `r round(df_summary_d_s$mean[1])` with a 90\% credible interval of [`r  round(df_summary_d_s$q10[1])`, `r  round(df_summary_d_s$q90[1])`] for the Separate model.

We can report the the posterior expectation for $\mu_1$ as `r round(df_summary_d_p$mean[1],1)` with a 90\% credible interval of [`r  round(df_summary_d_p$q10[1])`, `r  round(df_summary_d_p$q90[1],1)`] for the Pooled model.

We can report the the posterior expectation for $\mu_1$ as `r round(df_summary_d_h$mean[1],1)` with a 90\% credible interval of [`r  round(df_summary_d_h$q10[1])`, `r  round(df_summary_d_h$q90[1])`] for the Hierarchical model.

\newpage

# Appendix: Code for 2 d)

Separate model:

Here is the Stan code from \texttt{assignment7d\_sep.stan}

```{js}
data {
  int<lower=0> N;
  int<lower=0> J;
  vector[J] y[N];
}

parameters {
  vector[J] mu;
  vector<lower=0>[J] sigma;
}

model {
  // priors
  for (j in 1:J){
    mu[j] ~ normal(0, 10);
    sigma[j] ~ gamma(1, 1);
  }

  // likelihood
  for (j in 1:J)
    y[,j] ~ normal(mu[j], sigma[j]);
}

generated quantities {
  real ypred1;
  real ypred2;
  real ypred3;
  real ypred4;
  real ypred5;
  real ypred6;
  // Compute predictive distribution
  ypred1 = normal_rng(mu[1], sigma[1]);
  ypred2 = normal_rng(mu[2], sigma[2]);
  ypred3 = normal_rng(mu[3], sigma[3]);
  ypred4 = normal_rng(mu[4], sigma[4]);
  ypred5 = normal_rng(mu[5], sigma[5]);
  ypred6 = normal_rng(mu[6], sigma[6]);
}
```

Here is the R implementation

```{r, eval=FALSE}
options(mc.cores = parallel::detectCores())
sm_d_sep <- rstan::stan_model(file = "assignment7d_sep.stan")

stan_data_d_separate <- list(
  y = factory,
  N = nrow(factory),
  J = ncol(factory)
)

model_d_separate <- rstan::sampling(sm_d_sep, data = stan_data_d_separate)

m_s <- extract_variable_matrix(model_d_separate, variable = 'mu[1]')
q_s <- mcse_quantile(m_s, probs = c(0.10, 0.90))
              
df_summary_d_s <- summarise_draws(model_d_separate,mean,mcse_mean,
                                  ~quantile(.x, probs = c(0.1, 0.9))) %>% 
  filter(variable=='mu[1]') 
df_summary_d_s$mcse_q10 <- q_s[1]
df_summary_d_s$mcse_q90 <- q_s[2]

df_summary_d_s %>%  kbl(booktabs = T, linesep = "", caption = "Separate Model") %>%
  kable_styling(latex_options = c("striped", "scale_down", "HOLD_position")) 
```

Pooled model:

Here is the Stan code from \texttt{assignment7d\_pool.stan}

```{js}
data {
  int<lower=0> N;
  vector[N] y;
}

parameters {
  real mu;
  real<lower=0> sigma;
}

model {
  // priors
  mu ~ normal(0, 10);
  sigma ~ gamma(1, 1);

  // likelihood
  y ~ normal(mu, sigma);
}

generated quantities {
  real ypred;
  // Compute predictive distribution
  ypred = normal_rng(mu, sigma);
}
```

Here is the R implementation

```{r, eval=FALSE}
options(mc.cores = parallel::detectCores())
sm_d_pool <- rstan::stan_model(file = "assignment7d_pool.stan")

stan_data_d_pooled <- list(
  y = unname(unlist(factory)),
  N = nrow(factory) * ncol(factory)
)

model_d_pooled <- rstan::sampling(sm_d_pool, data = stan_data_d_pooled)

m_p <- extract_variable_matrix(model_d_pooled, variable = 'mu')
q_p <- mcse_quantile(m_p, probs = c(0.10, 0.90))
              
df_summary_d_p <- summarise_draws(model_d_pooled, mean,mcse_mean,
                                  ~quantile(.x, probs = c(0.1, 0.9))) %>% 
  filter(variable=='mu') 
df_summary_d_p$mcse_q10 <- q_p[1]
df_summary_d_p$mcse_q90 <- q_p[2]

df_summary_d_p %>%  kbl(booktabs = T, linesep = "", caption = "Pooled Model") %>%
  kable_styling(latex_options = c("striped", "scale_down", "HOLD_position")) 
```

Hierarchical model:

Here is the Stan code from \texttt{assignment7d\_h.stan}

```{js}
data {
  int<lower=0> N;
  int<lower=0> J;
  vector[J] y[N];
}

parameters {
  vector[J] mu;
  vector<lower=0>[J] sigma;
  real mu_p;
  real<lower=0> tau;
}

model {
  mu_p ~ normal(0, 10);
  tau ~ gamma(1, 1);
  // priors
  for (j in 1:J){
    mu[j] ~ normal(mu_p, tau);
    sigma[j] ~ gamma(1, 1);
  }

  // likelihood
  for (j in 1:J)
    y[,j] ~ normal(mu[j], sigma[j]);
}

generated quantities {
  real ypred1;
  real ypred2;
  real ypred3;
  real ypred4;
  real ypred5;
  real ypred6;
  real ypred7;
  real mu_pred7;
  real sigma_pred7;
  // Compute predictive distribution
  ypred1 = normal_rng(mu[1], sigma[1]);
  ypred2 = normal_rng(mu[2], sigma[2]);
  ypred3 = normal_rng(mu[3], sigma[3]);
  ypred4 = normal_rng(mu[4], sigma[4]);
  ypred5 = normal_rng(mu[5], sigma[5]);
  ypred6 = normal_rng(mu[6], sigma[6]);
  mu_pred7 = normal_rng(mu_p, tau);
  sigma_pred7 = inv_chi_square_rng(10);
  ypred7 = normal_rng(mu_pred7, sigma_pred7);
}
```

Here is the R implementation

```{r, eval=FALSE}
options(mc.cores = parallel::detectCores())
sm_d_h <- rstan::stan_model(file = "assignment7d_h.stan")

stan_data_d_h <- list(
  y = factory,
  N = nrow(factory),
  J = ncol(factory)
)

model_d_h <- rstan::sampling(sm_d_h, data = stan_data_d_h)

m_h <- extract_variable_matrix(model_d_h, variable = 'mu[1]')
q_h <- mcse_quantile(m_h, probs = c(0.10, 0.90))
              
df_summary_d_h <- summarise_draws(model_d_h,mean,mcse_mean,
                                  ~quantile(.x, probs = c(0.1, 0.9))) %>% 
  filter(variable=='mu[1]') 
df_summary_d_h$mcse_q10 <- q_h[1]
df_summary_d_h$mcse_q90 <- q_h[2]

df_summary_d_h %>%  kbl(booktabs = T, linesep = "", caption = "Hierarchical Model") %>%
  kable_styling(latex_options = c("striped", "scale_down", "HOLD_position")) 
```


